{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#19070001011\n",
        "#Bilge Kağan Şenol\n",
        "import nltk  # i import this for tokenization\n",
        "import shutil # i import this for clear the nltk for reinstall  because i got many problems that i choose wrong package to download before  so  i  clear  file directory everytime\n",
        "\n",
        "shutil.rmtree('/root/nltk_data', ignore_errors=True) #clear directory of nltk\n",
        "\n",
        "# `punkt` paketini yeniden indir56\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "_4IM4AoLf9tI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41422c83-76cd-464e-b28f-cfa2c08502f5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')   # it was  missing so i downloaded  but it is not necesser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CeE53fDgJAj",
        "outputId": "e163f1a4-cce5-48a5-f628-edffb1b9717f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from google.colab import drive     #data's are on the my drive\n",
        "drive.mount('/content/drive')         #connection with drive\n",
        "import os                                   #import os for  walk in  files\n",
        "from nltk.tokenize import word_tokenize     # we use tokenize feature from nltk\n",
        "nltk.download('punkt',force=True)      # necessary packages for nltk\n",
        "\n",
        "\n",
        "\n",
        "def tokenize (root_folder_path,tokenized):\n",
        "  for root, dirs, files in os.walk(root_folder_path):  # root: base file    dirs sub-files, files  are txt files that consist tweets\n",
        "    for file_name in files:\n",
        "        file_path = os.path.join(root, file_name)  # create pathway to file\n",
        "        with open(file_path, \"r\", encoding=\"latin-1\") as f:    #   here  normally  i need to use UTF-8 but when i use it  i got problem on the next line and i  can't fix it\n",
        "            text = f.read().strip()  # clean\n",
        "            tokens = word_tokenize(text)  # Tokenize et\n",
        "            tokenized.append(tokens)\n",
        "  return tokenized\n",
        "\n",
        "\n",
        "#here i  prepare file for positive tweets\n",
        "root_folder_path_positive = r\"/content/drive/My Drive/tweetforproject/Assignment-data/raw_texts/1\"  #paths\n",
        "tokenized_sentences_positive_tweets = []  #create a list for append txt by txt(tweet by tweet)\n",
        "tokenized_sentences_positive_tweets = tokenize(root_folder_path_positive,tokenized_sentences_positive_tweets)  #call the func\n",
        "#for negative tweets\n",
        "root_folder_path_negative = r\"/content/drive/My Drive/tweetforproject/Assignment-data/raw_texts/2\"                                         ############################ PLEASE İNSERT YOUR  DATA ON YOUR  DRİVE PATH ###########################################\n",
        "tokenized_sentences_negative_tweets = []\n",
        "tokenized_sentences_negative_tweets = tokenize(root_folder_path_negative,tokenized_sentences_negative_tweets)\n",
        "#for neutral tweets\n",
        "root_folder_path_neutral = r\"/content/drive/My Drive/tweetforproject/Assignment-data/raw_texts/3\"\n",
        "tokenized_sentences_neutral_tweets = []\n",
        "tokenized_sentences_neutral_tweets = tokenize(root_folder_path_neutral,tokenized_sentences_neutral_tweets)\n",
        "\n",
        "\n",
        "i = 0\n",
        "while i < 5:\n",
        "  print(tokenized_sentences_negative_tweets[i])\n",
        "  i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghok6FGJUvgS",
        "outputId": "c594a86f-4691-42c9-bfde-09e660f13234"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['výnn', 'kamu', 'paketine', 'üyeyim.Bir', 'mesaj', 'geldi', 've', 'Paket', 'aþým', 'ücretinden', 'bahsediyo', ',', 'abone', 'olurken', 'limit', 'bitiminde', 'otomatik', 'kesilecekdi', '?']\n",
            "['Madem', 'turkcell', 'hediye', '700', 'sms', 'verdi', ',', 'idareli', 'kullanmak', 'lazim', '\\x1a']\n",
            "['...', 'yada', 'ceremesini', 'ben', 'çekiyorum', '.', 'Sanýrým', 'artýk', '13', 'yýllýk', 'Turkcell', 'hizmetini', 'deðiþtirme', 'vakti', 'geldi', '.', 'Vadafone', 'haklý', 'galiba', '..', 'Pess', '....']\n",
            "['Dogru', 'bilgiyi', 'alabilmek', 'icin', 'ne', 'yapmam', 'lazim', 'illa', 'kavga', 'illa', 'siddet', 'off', 'turkcell', 'offf', '!', '!', '!', '!', '!']\n",
            "['Merhaba', '.', 'sizden', 'cevaplayip', '1', \"'\", 'e', 'bastiginiz', 'her', 'ödemeli', 'arama', 'için', '29krþ', 'ücret', 'aldigini', 'biliyor', 'muydunuz', '?', 'Artik', 'biliyorsunuz', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unicodedata\n",
        "\n",
        "# for normalize  the turkish word cause of reading with latin-1\n",
        "def normalize_turkish(word):\n",
        "    replacements = {\n",
        "        \"ý\": \"ı\",  # Türkçe'de \"ı\" harfine dönüşmeli\n",
        "        \"þ\": \"ş\",  # Türkçe'de \"ş\" harfine dönüşmeli\n",
        "        \"ð\": \"ğ\",  # Türkçe'de \"ğ\" harfine dönüşmeli\n",
        "        \"Ý\": \"İ\",  # Büyük harf \"İ\"\n",
        "        \"Þ\": \"Ş\",  # Büyük harf \"Ş\"\n",
        "        \"Ð\": \"Ğ\",  # Büyük harf \"Ğ\"\n",
        "    }\n",
        "    for old, new in replacements.items():\n",
        "        word = word.replace(old, new)\n",
        "    return word\n",
        "    ##########################################################################\n",
        "def normalize_turkish_sentences(tokenized_sentences):          #function for list\n",
        "    for i , sentence in enumerate(tokenized_sentences):\n",
        "      tokenized_sentences[i] = [normalize_turkish(word) for word in sentence]\n",
        "    return tokenized_sentences\n",
        "\n",
        "tokenized_sentences_positive_tweets = normalize_turkish_sentences(tokenized_sentences_positive_tweets)\n",
        "tokenized_sentences_negative_tweets = normalize_turkish_sentences(tokenized_sentences_negative_tweets)      # call the function for all classes\n",
        "tokenized_sentences_neutral_tweets = normalize_turkish_sentences(tokenized_sentences_neutral_tweets)\n",
        "\n",
        "i=0\n",
        "while i<5:\n",
        "  print(tokenized_sentences_positive_tweets[i])\n",
        "  i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbnn710dxFm6",
        "outputId": "debad6e7-97d6-4b57-bf83-172d8d7e3423"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sene', 'önce', 'turkcell', 'reklemi', 'vardı', '9', 'hazırız', 'bekliyoruz', 'diye', 'ne', 'çabuk', 'gecmis', 'simdi', 'de', 'çabuk', 'gecer', 'umarimm']\n",
            "['hayat', 'yeni', 'yıl', 'mutlulugunu', 'paylaşınca', 'güzellll']\n",
            "['hizliyim', 'ama', 'hayirsiz', 'degilimm', 'bencede', 'goruselim', 'turkcell', 'boy']\n",
            "['kötü', 'espiri', 'yapanları', 'seçiyorlar', 'yaw', 'nokiada', 'hafta', 'boyunca', 'ne', 'çektiğimi', 'gördün', 'sn', 'turkcell', 'dakikada', 'hediye', 'etti']\n",
            "['tesekkurler', 'turkcell']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##################################################### NOT NECESSARY  I USE THIS FOR TESTING PURPOSE  ##############################################################\n",
        "turkcell_elestirileri = [\n",
        "    \"Turkcell internet hızı eskiden iyiydi ama şimdi yavaşlamış, anlamıyorum bu kadar paraya neden böle bir hizmet sunuyorsunuz?\",\n",
        "    \"Faturamı kontrol ederken eksik veya fazladan ücret kesilmiş, müşteri hizmetleri de ilgisiz, nasıl düzelcek bu?\",\n",
        "    \"Turkcell çekim gücü şehir içinde bile sorunlu, köyde olsak hiç çekmeyecek sanırım.\",\n",
        "    \"Uygulamanız sürekli çöküyor, ne zaman düzeltme yapıcaksınız, herkes mi aynı sorunları yaşıyo?\",\n",
        "    \"Turkcell kampanyaları çok pahalı ve avantajlı da değil, başak firmalar daha uygun hizmet veriyor!\"\n",
        "]\n",
        "tokenized_test = []\n",
        "\n",
        "for cümle in turkcell_elestirileri:\n",
        "    tokens = word_tokenize(cümle)  # Tokenize işlemi\n",
        "    tokenized_test.append(tokens)  # Tokenize edilmiş cümleyi listeye ekle\n",
        "\n",
        "# Sonuçları yazdır\n",
        "for i, tokens in enumerate(tokenized_test):\n",
        "    print(f\"Cümle {i+1} Tokenized: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQb2F30tUqPT",
        "outputId": "e151717e-4b1b-4fcc-92f1-5c8507c5e9fd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cümle 1 Tokenized: ['Turkcell', 'internet', 'hızı', 'eskiden', 'iyiydi', 'ama', 'şimdi', 'yavaşlamış', ',', 'anlamıyorum', 'bu', 'kadar', 'paraya', 'neden', 'böle', 'bir', 'hizmet', 'sunuyorsunuz', '?']\n",
            "Cümle 2 Tokenized: ['Faturamı', 'kontrol', 'ederken', 'eksik', 'veya', 'fazladan', 'ücret', 'kesilmiş', ',', 'müşteri', 'hizmetleri', 'de', 'ilgisiz', ',', 'nasıl', 'düzelcek', 'bu', '?']\n",
            "Cümle 3 Tokenized: ['Turkcell', 'çekim', 'gücü', 'şehir', 'içinde', 'bile', 'sorunlu', ',', 'köyde', 'olsak', 'hiç', 'çekmeyecek', 'sanırım', '.']\n",
            "Cümle 4 Tokenized: ['Uygulamanız', 'sürekli', 'çöküyor', ',', 'ne', 'zaman', 'düzeltme', 'yapıcaksınız', ',', 'herkes', 'mi', 'aynı', 'sorunları', 'yaşıyo', '?']\n",
            "Cümle 5 Tokenized: ['Turkcell', 'kampanyaları', 'çok', 'pahalı', 've', 'avantajlı', 'da', 'değil', ',', 'başak', 'firmalar', 'daha', 'uygun', 'hizmet', 'veriyor', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########  NOT NECESSARY #######################\n",
        "i = 0\n",
        "while i<5:\n",
        "  print(tokenized_sentences_positive_tweets[i])\n",
        "  i += 1"
      ],
      "metadata": {
        "id": "BSNXH8ofQWbG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f16254f-6117-4cd3-f41d-c248a519ed50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['3', 'sene', 'önce', 'Turkcell', 'reklemi', 'vardı', '2', '0', '0', '9', 'hazırız', 'bekliyoruz', 'diye', ':', ')', 'ne', 'çabuk', 'gecmis', ',', 'simdi', 'de', 'çabuk', 'gecer', 'umarimm']\n",
            "['hayat', 'yeni', 'yıl', 'mutlulugunu', 'paylaşınca', 'güzellll', ':', ')', ')', ')']\n",
            "['hizliyim', 'ama', 'hayirsiz', 'degilimm', ':', ')', 'bencede', 'goruselim', ':', ')', 'turkcell', 'boy', ':', ')']\n",
            "['kötü', 'espiri', 'yapanları', 'seçiyorlar', 'yaw', 'nokiada', '3', 'hafta', 'boyunca', 'ne', 'çektiğimi', 'gördün', 'sn', 'turkcell', '15', 'dakikada', 'hediye', 'etti', ':', ')']\n",
            "['Tesekkurler', 'Turkcell']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "                                                    #NOW  I AM STARTING TO CLEAN DATA\n",
        "def lower_case_tokens(tokenized_sentences):     #for lower case operation\n",
        "  for i , sentence in enumerate(tokenized_sentences):     # we walk in index for using txt files one by one sentence = one tweet we use it for all functions\n",
        "    tokenized_sentences[i] = [word.lower() for word in sentence]\n",
        "  return tokenized_sentences\n",
        "\n",
        "def remove_punctiation (tokenized_sentences):      # i need to get rid of punctiation for clean analyse\n",
        "    for sentence in tokenized_sentences:\n",
        "      for word in sentence:\n",
        "        if word in string.punctuation:\n",
        "          sentence.remove(word)\n",
        "    return tokenized_sentences\n",
        "\n",
        "def remove_numbers (tokenized_sentences):           # cleaning numbers\n",
        "  for sentence in tokenized_sentences:\n",
        "    for word in sentence:\n",
        "      if word.isdigit():\n",
        "        sentence.remove(word)\n",
        "  return tokenized_sentences\n",
        "\n",
        "def remove_empty_tokens(tokenized_sentences):        #while tokenize we can have many empty tokens so we need to clean this\n",
        "  for sentence in tokenized_sentences:\n",
        "    for word in sentence:\n",
        "      if word == '':\n",
        "        sentence.remove(word)\n",
        "  return tokenized_sentences\n",
        "\n",
        "def remove_paranthesis(tokenized_sentences):                       # i don't know is it necessary but when i research i found out we need to get rid of paranthesis i cant find a library for it so i manualy do it.\n",
        "  for sentence in tokenized_sentences:\n",
        "    for word in sentence:\n",
        "      if word in ('(', ')', '[', ']', '{', '}'):\n",
        "        sentence.remove(word)\n",
        "  return tokenized_sentences\n",
        "\n",
        "#POSİTİVE\n",
        "tokenized_sentences_positive_tweets = lower_case_tokens(tokenized_sentences_positive_tweets)\n",
        "tokenized_sentences_positive_tweets = remove_punctiation(tokenized_sentences_positive_tweets)\n",
        "tokenized_sentences_positive_tweets = remove_numbers(tokenized_sentences_positive_tweets)\n",
        "tokenized_sentences_positive_tweets = remove_empty_tokens(tokenized_sentences_positive_tweets)\n",
        "tokenized_sentences_positive_tweets = remove_paranthesis(tokenized_sentences_positive_tweets)\n",
        "\n",
        "#NEGATİVE                                                                                              # I CALL EVERY FUNCTION FOR MY ALL CLASSES\n",
        "tokenized_sentences_negative_tweets = lower_case_tokens(tokenized_sentences_negative_tweets)\n",
        "tokenized_sentences_negative_tweets = remove_punctiation(tokenized_sentences_negative_tweets)\n",
        "tokenized_sentences_negative_tweets = remove_numbers(tokenized_sentences_negative_tweets)\n",
        "tokenized_sentences_negative_tweets = remove_empty_tokens(tokenized_sentences_negative_tweets)\n",
        "tokenized_sentences_negative_tweets = remove_paranthesis(tokenized_sentences_negative_tweets)\n",
        "\n",
        "#NEUTRAL\n",
        "\n",
        "tokenized_sentences_neutral_tweets = lower_case_tokens(tokenized_sentences_neutral_tweets)\n",
        "tokenized_sentences_neutral_tweets = remove_punctiation(tokenized_sentences_neutral_tweets)\n",
        "tokenized_sentences_neutral_tweets = remove_numbers(tokenized_sentences_neutral_tweets)\n",
        "tokenized_sentences_neutral_tweets = remove_empty_tokens(tokenized_sentences_neutral_tweets)\n",
        "tokenized_sentences_neutral_tweets = remove_paranthesis(tokenized_sentences_neutral_tweets)\n",
        "\n",
        "####test####\n",
        "tokenized_test = lower_case_tokens(tokenized_test)\n",
        "tokenized_test = remove_punctiation(tokenized_test)\n",
        "tokenized_test = remove_numbers(tokenized_test)\n",
        "i = 0\n",
        "while i<5:\n",
        "  print(tokenized_test[i])\n",
        "  i += 1\n",
        "i = 0\n",
        "while i<5:\n",
        "  print(tokenized_sentences_positive_tweets[i])\n",
        "  i += 1"
      ],
      "metadata": {
        "id": "gqwBjj4fKkwD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4080c12c-af1c-4e4d-a51a-f00dbd670d2f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['turkcell', 'internet', 'hızı', 'eskiden', 'iyiydi', 'ama', 'şimdi', 'yavaşlamış', 'anlamıyorum', 'bu', 'kadar', 'paraya', 'neden', 'böle', 'bir', 'hizmet', 'sunuyorsunuz']\n",
            "['faturamı', 'kontrol', 'ederken', 'eksik', 'veya', 'fazladan', 'ücret', 'kesilmiş', 'müşteri', 'hizmetleri', 'de', 'ilgisiz', 'nasıl', 'düzelcek', 'bu']\n",
            "['turkcell', 'çekim', 'gücü', 'şehir', 'içinde', 'bile', 'sorunlu', 'köyde', 'olsak', 'hiç', 'çekmeyecek', 'sanırım']\n",
            "['uygulamanız', 'sürekli', 'çöküyor', 'ne', 'zaman', 'düzeltme', 'yapıcaksınız', 'herkes', 'mi', 'aynı', 'sorunları', 'yaşıyo']\n",
            "['turkcell', 'kampanyaları', 'çok', 'pahalı', 've', 'avantajlı', 'da', 'değil', 'başak', 'firmalar', 'daha', 'uygun', 'hizmet', 'veriyor']\n",
            "['sene', 'önce', 'turkcell', 'reklemi', 'vardı', 'hazırız', 'bekliyoruz', 'diye', 'ne', 'çabuk', 'gecmis', 'simdi', 'de', 'çabuk', 'gecer', 'umarimm']\n",
            "['hayat', 'yeni', 'yıl', 'mutlulugunu', 'paylaşınca', 'güzellll']\n",
            "['hizliyim', 'ama', 'hayirsiz', 'degilimm', 'bencede', 'goruselim', 'turkcell', 'boy']\n",
            "['kötü', 'espiri', 'yapanları', 'seçiyorlar', 'yaw', 'nokiada', 'hafta', 'boyunca', 'ne', 'çektiğimi', 'gördün', 'sn', 'turkcell', 'dakikada', 'hediye', 'etti']\n",
            "['tesekkurler', 'turkcell']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  https://github.com/sgsinclair/trombone/blob/master/src/main/resources/org/voyanttools/trombone/keywords/stop.tr.turkish-lucene.txt\n",
        "#I found the turkish stopword list from this github link\n",
        "turkish_stopwords = [\n",
        "    \"acaba\", \"altmış\", \"altı\", \"ama\", \"ancak\", \"arada\", \"aslında\", \"ayrıca\", \"bana\",\n",
        "    \"bazı\", \"belki\", \"ben\", \"benden\", \"beni\", \"benim\", \"beri\", \"beş\", \"bile\", \"bin\",\n",
        "    \"bir\", \"birçok\", \"biri\", \"birkaç\", \"birkez\", \"birşey\", \"birşeyi\", \"biz\", \"bize\",\n",
        "    \"bizden\", \"bizi\", \"bizim\", \"böyle\", \"böylece\", \"bu\", \"buna\", \"bunda\", \"bundan\",\n",
        "    \"bunlar\", \"bunları\", \"bunların\", \"bunu\", \"bunun\", \"burada\", \"çok\", \"çünkü\", \"da\",\n",
        "    \"daha\", \"dahi\", \"de\", \"defa\", \"değil\", \"diğer\", \"diye\", \"doksan\", \"dokuz\", \"dolayı\",\n",
        "    \"dolayısıyla\", \"dört\", \"edecek\", \"eden\", \"ederek\", \"edilecek\", \"ediliyor\", \"edilmesi\",\n",
        "    \"ediyor\", \"eğer\", \"elli\", \"en\", \"etmesi\", \"etti\", \"ettiği\", \"ettiğini\", \"gibi\", \"göre\",\n",
        "    \"halen\", \"hangi\", \"hatta\", \"hem\", \"henüz\", \"hep\", \"hepsi\", \"her\", \"herhangi\",\n",
        "    \"herkesin\", \"hiç\", \"hiçbir\", \"için\", \"iki\", \"ile\", \"ilgili\", \"ise\", \"işte\", \"itibaren\",\n",
        "    \"itibariyle\", \"kadar\", \"karşın\", \"katrilyon\", \"kendi\", \"kendilerine\", \"kendini\",\n",
        "    \"kendisi\", \"kendisine\", \"kendisini\", \"kez\", \"ki\", \"kim\", \"kimden\", \"kime\", \"kimi\",\n",
        "    \"kimse\", \"kırk\", \"milyar\", \"milyon\", \"mu\", \"mü\", \"mı\", \"nasıl\", \"ne\", \"neden\",\n",
        "    \"nedenle\", \"nerde\", \"nerede\", \"nereye\", \"niye\", \"niçin\", \"o\", \"olan\", \"olarak\",\n",
        "    \"oldu\", \"olduğu\", \"olduğunu\", \"olduklarını\", \"olmadı\", \"olmadığı\", \"olmak\",\n",
        "    \"olması\", \"olmayan\", \"olmaz\", \"olsa\", \"olsun\", \"olup\", \"olur\", \"olursa\", \"oluyor\",\n",
        "    \"on\", \"ona\", \"ondan\", \"onlar\", \"onlardan\", \"onları\", \"onların\", \"onu\", \"onun\",\n",
        "    \"otuz\", \"oysa\", \"öyle\", \"pek\", \"rağmen\", \"sadece\", \"sanki\", \"sekiz\", \"seksen\",\n",
        "    \"sen\", \"senden\", \"seni\", \"senin\", \"siz\", \"sizden\", \"sizi\", \"sizin\", \"şey\",\n",
        "    \"şeyden\", \"şeyi\", \"şeyler\", \"şöyle\", \"şu\", \"şuna\", \"şunda\", \"şundan\", \"şunları\",\n",
        "    \"şunu\", \"tarafından\", \"trilyon\", \"tüm\", \"üç\", \"üzere\", \"var\", \"vardı\", \"ve\",\n",
        "    \"veya\", \"ya\", \"yani\", \"yapacak\", \"yapılan\", \"yapılması\", \"yapıyor\", \"yapmak\",\n",
        "    \"yaptı\", \"yaptığı\", \"yaptığını\", \"yaptıkları\", \"yedi\", \"yerine\", \"yetmiş\", \"yine\",\n",
        "    \"yirmi\", \"yoksa\", \"yüz\", \"zaten\"\n",
        "]\n",
        "\n",
        "def remove_stopwords(tokenized_sentences):                         #when it  is match with the stopword we get rid of it\n",
        "    for sentence in tokenized_sentences:\n",
        "        sentence[:] = [word for word in sentence if word.lower() not in turkish_stopwords]\n",
        "    return tokenized_sentences\n",
        "\n",
        "tokenized_sentences_positive_tweets = remove_stopwords(tokenized_sentences_positive_tweets)                #CALL  FUNCTION FOR ALL LİSTS\n",
        "tokenized_sentences_negative_tweets = remove_stopwords(tokenized_sentences_negative_tweets)\n",
        "tokenized_sentences_neutral_tweets = remove_stopwords(tokenized_sentences_neutral_tweets)\n",
        "\n",
        "\n",
        "\n",
        "i = 0\n",
        "while i<5:\n",
        "  print(tokenized_sentences_negative_tweets[i])\n",
        "  i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uD3GP8BQB6o",
        "outputId": "326d5380-e27e-4761-8193-0a08daa13f24"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['vınn', 'kamu', 'paketine', 'üyeyim.bir', 'mesaj', 'geldi', 'paket', 'aşım', 'ücretinden', 'bahsediyo', 'abone', 'olurken', 'limit', 'bitiminde', 'otomatik', 'kesilecekdi']\n",
            "['madem', 'turkcell', 'hediye', 'sms', 'verdi', 'idareli', 'kullanmak', 'lazim', '\\x1a']\n",
            "['...', 'yada', 'ceremesini', 'çekiyorum', 'sanırım', 'artık', 'yıllık', 'turkcell', 'hizmetini', 'değiştirme', 'vakti', 'geldi', 'vadafone', 'haklı', 'galiba', '..', 'pess', '....']\n",
            "['dogru', 'bilgiyi', 'alabilmek', 'icin', 'yapmam', 'lazim', 'illa', 'kavga', 'illa', 'siddet', 'off', 'turkcell', 'offf']\n",
            "['merhaba', 'cevaplayip', 'e', 'bastiginiz', 'ödemeli', 'arama', '29krş', 'ücret', 'aldigini', 'biliyor', 'muydunuz', 'artik', 'biliyorsunuz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##https://github.com/brolin59/trnlp\n",
        "## here is the source\n",
        "!pip install trnlp          ## i prefer TRNLP  for lemmatization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKK9zL6vDiG5",
        "outputId": "fc1c8f24-d589-43bc-a45b-7c6c321a0eea"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: trnlp in /usr/local/lib/python3.10/dist-packages (0.2.3a0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trnlp import TrnlpWord\n",
        "obj = TrnlpWord()\n",
        "\n",
        "def lemmatize(tokenized_sentences):\n",
        "    lemmatized_sentences = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        lemmatized_tokens = []\n",
        "        for word in sentence:\n",
        "            word = normalize_turkish(word)  # i write this again for exception on the data\n",
        "            if isinstance(word, str) and word.strip():  # control for just not empty\n",
        "                try:\n",
        "                    if word.isalpha() and len(word) > 1:  # meaningfull words that enough to create a word\n",
        "                        obj.setword(word)\n",
        "                        lemma = obj.get_base\n",
        "                        # lemmatization\n",
        "                        if len(lemma) > 2:\n",
        "                            lemmatized_tokens.append(lemma)                # here for checking lemma is too short or not. because  if it is less or equal  2 we make the lemma original version\n",
        "                        else:\n",
        "                            lemmatized_tokens.append(word)  #here is for original\n",
        "                    else:\n",
        "                        lemmatized_tokens.append(word)  # numbers or not proper tokens\n",
        "                except Exception as e:\n",
        "                    print(f\"Hata oluştu: {word} - {e}\")\n",
        "                    lemmatized_tokens.append(word)  # checking mistakes and then continue to put back\n",
        "            else:\n",
        "                print(f\"Atlanıyor: {word}\")        #skipping if word is not proper\n",
        "                lemmatized_tokens.append(word)\n",
        "        lemmatized_sentences.append(lemmatized_tokens)\n",
        "    return lemmatized_sentences\n",
        "\n",
        "tokenized_sentences_positive_tweets = lemmatize(tokenized_sentences_positive_tweets)\n",
        "tokenized_sentences_negative_tweets = lemmatize(tokenized_sentences_negative_tweets)       #CALL FUNC FOR ALL CLASSES\n",
        "tokenized_sentences_neutral_tweets = lemmatize(tokenized_sentences_neutral_tweets)\n",
        "i = 0\n",
        "while i<5:\n",
        "  print(tokenized_sentences_negative_tweets[i])\n",
        "  i += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKrSDdbtDfhX",
        "outputId": "be5bf186-e068-42c3-fbd7-4f61df5e7bbb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['vınn', 'kamu', 'paket', 'üyeyim.bir', 'mesaj', 'gel', 'paket', 'aşım', 'ücret', 'bahset', 'abone', 'olur', 'limit', 'bit', 'otomatik', 'kesilecekdi']\n",
            "['madem', 'Turkcell', 'hediye', 'sms', 'verdi', 'idare', 'kullan', 'lazim', '\\x1a']\n",
            "['...', 'yad', 'cereme', 'çek', 'san', 'art', 'yıl', 'Turkcell', 'hizmet', 'değ', 'vakit', 'gel', 'vadafone', 'hak', 'galiba', '..', 'pess', '....']\n",
            "['dogru', 'bilgi', 'alabilmek', 'icin', 'yap', 'lazim', 'illa', 'kavga', 'illa', 'siddet', 'off', 'Turkcell', 'offf']\n",
            "['merhaba', 'cevaplayip', 'e', 'bastiginiz', 'öde', 'ara', '29krş', 'ücret', 'aldigini', 'bile', 'muydunuz', 'artik', 'bile']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## not necessary just  compare with current #####################\n",
        "\n",
        "tokenized_test = lemmatize(tokenized_test)\n",
        "i = 0\n",
        "while i<5:\n",
        "  print(tokenized_test[i])\n",
        "  i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aGEirqHVXol",
        "outputId": "e5fb2143-59cc-4261-8592-f1cacf613273"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Turkcell', 'internet', 'hız', 'eskiden', 'iyi', 'ama', 'şimdi', 'yavaş', 'anlamıyorum', 'bu', 'kadar', 'para', 'neden', 'böle', 'bir', 'hizmet', 'sun']\n",
            "['fatura', 'kontrol', 'eder', 'eksik', 'veya', 'fazladan', 'ücret', 'kesi', 'Müşteri', 'hizmet', 'de', 'ilgi', 'nasıl', 'düzelcek', 'bu']\n",
            "['Turkcell', 'çek', 'gücü', 'şehir', 'içinde', 'bile', 'sorun', 'köy', 'olsak', 'hiç', 'çek', 'san']\n",
            "['uygula', 'sürek', 'çök', 'ne', 'zaman', 'düzel', 'yapı', 'herkes', 'mi', 'aynı', 'sorun', 'yaşa']\n",
            "['Turkcell', 'kampanya', 'çok', 'paha', 've', 'avantaj', 'da', 'değil', 'başak', 'firma', 'daha', 'uygun', 'hizmet', 'ver']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########not necessary#############\n",
        "\n",
        "tokenized_test  = remove_stopwords(tokenized_test)\n",
        "i = 0\n",
        "while i<5:\n",
        "  print(tokenized_test[i])\n",
        "  i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhwaiztrQwMI",
        "outputId": "75bfa455-fc3c-41fa-9423-001eda987aaf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Turkcell', 'internet', 'hız', 'eskiden', 'iyi', 'şimdi', 'yavaş', 'anlamıyorum', 'para', 'böle', 'hizmet', 'sun']\n",
            "['fatura', 'kontrol', 'eder', 'eksik', 'fazladan', 'ücret', 'kesi', 'Müşteri', 'hizmet', 'ilgi', 'düzelcek']\n",
            "['Turkcell', 'çek', 'gücü', 'şehir', 'içinde', 'sorun', 'köy', 'olsak', 'çek', 'san']\n",
            "['uygula', 'sürek', 'çök', 'zaman', 'düzel', 'yapı', 'herkes', 'mi', 'aynı', 'sorun', 'yaşa']\n",
            "['Turkcell', 'kampanya', 'paha', 'avantaj', 'başak', 'firma', 'uygun', 'hizmet', 'ver']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF CALCULATION\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer    # I use sckitlearn library for  calculation of tf-idf\n",
        "import pandas as pd  #for reading data i  use pandas library\n",
        "# i turn string every list  but its still seperated\n",
        "positive_texts = [' '.join(sentence) for sentence in tokenized_sentences_positive_tweets]\n",
        "negative_texts = [' '.join(sentence) for sentence in tokenized_sentences_negative_tweets]\n",
        "neutral_texts = [' '.join(sentence) for sentence in tokenized_sentences_neutral_tweets]\n",
        "\n",
        "# i combine all tweets in one and   give label for seeing classes\n",
        "all_texts = positive_texts + negative_texts + neutral_texts\n",
        "labels = [1] * len(positive_texts) + [2] * len(negative_texts) + [3] * len(neutral_texts)\n",
        "\n",
        "# calculation of tf-idf\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
        "\n",
        "# turned to the dataframe\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "df_tfidf['class'] = labels  # i add classes\n",
        "\n",
        "# we build a csv file\n",
        "df_tfidf.to_csv(\"tfidf_with_classes.csv\", index=False)\n",
        "\n",
        "# we can download it  now  csv you can find out it on the folder too\n",
        "from google.colab import files\n",
        "files.download(\"tfidf_with_classes.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "h7gc3d5wW2xE",
        "outputId": "aef8d6ba-e7f4-428f-9b2c-26e88c0d2757"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_36fe377c-53c1-4294-8eaf-8361288b3fbc\", \"tfidf_with_classes.csv\", 92563416)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TİME FOR FITTING A MODEL WITH DATA AND CALCULATE MANUALLY ALL METRİCS THAT WANTS\n",
        "from sklearn.model_selection import StratifiedKFold    # it is for starified  10 folds  cross validation\n",
        "from sklearn.neighbors import KNeighborsClassifier    # we import knn classifier\n",
        "import numpy as np                                  # we calcuate metrics manually so we need to numpy for math operation\n",
        "\n",
        "# we prepare X (data)  and y (label) targets for class.\n",
        "X = tfidf_matrix.toarray()\n",
        "y = np.array(labels)\n",
        "\n",
        "# Stratified 10-Folds Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)  #we use it when we split the data\n",
        "\n",
        "# manuel calculation of the metrics\n",
        "def calculate_metrics_with_totals(y_true, y_pred, classes):\n",
        "    metrics = {}\n",
        "    totals = {\"TP\": {}, \"FP\": {}, \"FN\": {}}\n",
        "    for cls in classes:\n",
        "        if cls not in metrics:\n",
        "            metrics[cls] = {}  # if key is empty we create an empty word\n",
        "        tp = sum((y_true == cls) & (y_pred == cls))  # true positive\n",
        "        fp = sum((y_true != cls) & (y_pred == cls))  # false positive\n",
        "        fn = sum((y_true == cls) & (y_pred != cls))  # false negative\n",
        "        tn = sum((y_true != cls) & (y_pred != cls))  # true negative\n",
        "\n",
        "        # Save totals\n",
        "        totals[\"TP\"][cls] = tp\n",
        "        totals[\"FP\"][cls] = fp\n",
        "        totals[\"FN\"][cls] = fn\n",
        "\n",
        "        # Calculate metrics\n",
        "        precision = tp / (tp + fp) if tp + fp > 0 else 0  # precision calculation\n",
        "        recall = tp / (tp + fn) if tp + fn > 0 else 0  # recall calculation\n",
        "        f_score = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0  # f-score calculation\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn) if tp + tn + fp + fn > 0 else 0  # accuracy calculation\n",
        "\n",
        "        metrics[cls][\"precision\"] = precision\n",
        "        metrics[cls][\"recall\"] = recall  # save all metrics\n",
        "        metrics[cls][\"f_score\"] = f_score\n",
        "        metrics[cls][\"accuracy\"] = accuracy\n",
        "    return metrics, totals\n",
        "\n",
        "# try k values\n",
        "k_values = [20,21,22,23]\n",
        "for k in k_values:\n",
        "    metrics_per_fold = []\n",
        "    totals_per_fold = {\"TP\": [], \"FP\": [], \"FN\": []}\n",
        "    print(f\"\\nTesting for k={k}\")\n",
        "    for train_index, test_index in skf.split(X, y):  # time to use it skf\n",
        "        X_train, X_test = X[train_index], X[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "        # k-NN modeli\n",
        "        knn = KNeighborsClassifier(n_neighbors=k, metric='cosine')\n",
        "        knn.fit(X_train, y_train)\n",
        "\n",
        "        # Tahmin yap\n",
        "        y_pred = knn.predict(X_test)\n",
        "\n",
        "        # calculate performance metric and totals\n",
        "        metrics, totals = calculate_metrics_with_totals(y_test, y_pred, classes=np.unique(y))\n",
        "        metrics_per_fold.append(metrics)\n",
        "        for key in [\"TP\", \"FP\", \"FN\"]:\n",
        "            totals_per_fold[key].append(totals[key])\n",
        "\n",
        "    # calculation avarage metrics\n",
        "    avg_metrics = {}\n",
        "    for cls in np.unique(y):\n",
        "        if cls not in avg_metrics:\n",
        "            avg_metrics[cls] = {}  # create an empty dictionary if the key does not exist\n",
        "        for metric in ['precision', 'recall', 'f_score', 'accuracy']:\n",
        "            avg_metrics[cls][metric] = np.mean([fold[cls][metric] for fold in metrics_per_fold])\n",
        "\n",
        "    # combine totals\n",
        "    final_totals = {key: {cls: sum(fold[cls] for fold in totals_per_fold[key]) for cls in np.unique(y)} for key in [\"TP\", \"FP\", \"FN\"]}\n",
        "\n",
        "    # saving result\n",
        "    results[k] = {\"metrics\": avg_metrics, \"totals\": final_totals}\n",
        "\n",
        "# print results\n",
        "for k, result in results.items():\n",
        "    print(f\"\\nMetrics and Totals for k={k}:\")\n",
        "    for cls, metric_values in result[\"metrics\"].items():\n",
        "        print(f\"Class {cls}: Metrics = {metric_values}\")\n",
        "    for key, class_totals in result[\"totals\"].items():\n",
        "        print(f\"Total {key}: {class_totals}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPF7OEGFhRmI",
        "outputId": "a94f754d-4b77-449e-b9c0-29f631e76cbb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing for k=20\n",
            "\n",
            "Metrics and Totals for k=80:\n",
            "Class 1: Metrics = {'precision': 0.5225131520730024, 'recall': 0.39696491228070174, 'f_score': 0.4502984638308759, 'accuracy': 0.7573333333333333}\n",
            "Class 2: Metrics = {'precision': 0.5262583873487103, 'recall': 0.8360828488372093, 'f_score': 0.6458165536269601, 'accuracy': 0.6066666666666667}\n",
            "Class 3: Metrics = {'precision': 0.5150323394340978, 'recall': 0.20378289473684214, 'f_score': 0.2900241740015496, 'accuracy': 0.6833333333333333}\n",
            "Total TP: {1: 300, 2: 1076, 3: 195}\n",
            "Total FP: {1: 272, 2: 969, 3: 188}\n",
            "Total FN: {1: 456, 2: 211, 3: 762}\n",
            "\n",
            "Metrics and Totals for k=20:\n",
            "Class 1: Metrics = {'precision': 0.47318924442873855, 'recall': 0.5079298245614036, 'f_score': 0.4890331996134655, 'accuracy': 0.7326666666666668}\n",
            "Class 2: Metrics = {'precision': 0.5847792088591369, 'recall': 0.6992732558139536, 'f_score': 0.6367331814146711, 'accuracy': 0.6576666666666666}\n",
            "Class 3: Metrics = {'precision': 0.5080513761157097, 'recall': 0.3438048245614035, 'f_score': 0.40913784300896827, 'accuracy': 0.6849999999999999}\n",
            "Total TP: {1: 384, 2: 900, 3: 329}\n",
            "Total FP: {1: 430, 2: 640, 3: 317}\n",
            "Total FN: {1: 372, 2: 387, 3: 628}\n"
          ]
        }
      ]
    }
  ]
}